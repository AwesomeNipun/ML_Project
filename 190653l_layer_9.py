# -*- coding: utf-8 -*-
"""190653L_Layer_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lod12UI27bNuSV9VPC7076l4VgdPJnbc

## Imports
"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.decomposition import PCA
from sklearn.preprocessing import RobustScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from catboost import CatBoostClassifier
from imblearn.over_sampling import RandomOverSampler

"""## Data Retrieval"""

# Specify the file paths
train_path = "./train.csv"
valid_path = "./valid.csv"
test_path = "./test.csv"

# Load the CSV files into Pandas DataFrames
train = pd.read_csv(train_path)
valid = pd.read_csv(valid_path)
test = pd.read_csv(test_path)

"""## Preprocessing"""

train_ = train.copy()
valid_ = valid.copy()
test_ = test.copy()
test_ = test_.drop(columns=['ID'])

labels = ['label_1', 'label_2', 'label_3', 'label_4']

# dict to store train and valid dataset for each label
train_features = {}
train_label = {}
valid_features = {}
valid_label = {}

# seperate the train and valid datasets into labels
# remove the rows where label is NaN

for label in labels:
    # Filter rows where the specific label column does not have missing values
    train_temp = train_[train_[label].notna()]
    valid_temp = valid_[valid_[label].notna()]

    # Separate the features and labels
    train_features[label] = train_temp.drop(labels, axis=1)
    train_label[label] = train_temp[label]
    valid_features[label] = valid_temp.drop(labels, axis=1)
    valid_label[label] = valid_temp[label]

"""## Common Methods

### Classifier
"""

models = ["svm", "rfc", "lr", "cb", "knn"]

def classify(model, L, X_train, X_valid, y_train, y_valid):
  classifier = None
  class_weight = None if L == 'label_1' else 'balanced'

  if model == "svm":
    classifier = SVC(kernel='linear', class_weight=class_weight)

  elif model == "rfc":
    classifier = RandomForestClassifier(class_weight=class_weight)

  elif model == "lr":
    classifier = LogisticRegression(class_weight=class_weight, max_iter=1000)

  elif model == "cb":
    classifier = CatBoostClassifier(class_weights='Auto', iterations=100)

  elif model == "knn":
    classifier = KNeighborsClassifier(n_neighbors=1)



  if model == "knn": # KNN
    classifier.fit(np.array(X_train[L]), y_train[L])
    y_pred = classifier.predict(np.array(X_valid[L]))
  else:
    classifier.fit(X_train[L], y_train[L])
    y_pred = classifier.predict(X_valid[L])

  accuracy = accuracy_score(y_valid[L], y_pred)
  print(f"{model} Accuracy Score for {L} = %.2f%%" % (accuracy * 100))

  return accuracy

"""### Random grid search with cross validation"""

def random_grid_search_classifier(classifier, X_train, y_train, param_distributions):

    # Create a random search object with cross-validation
    random_search = RandomizedSearchCV(
        estimator=classifier,
        param_distributions=param_distributions,
        scoring='accuracy',
        cv=2,
        verbose=1,
        n_jobs=-1,
        n_iter=5,
        random_state=42)

    # Fit the random search to the training data
    random_search.fit(X_train, y_train)

    # Get the best model from the random search
    best_model = random_search.best_estimator_
    best_params = random_search.best_params_
    best_accuracy = random_search.best_score_

    return best_model, best_accuracy, best_params

"""### Grid Search with cross validation"""

def grid_search_classifier(classifier, X_train, y_train, X_valid, y_valid, param_grid):

    # Create a grid search object with cross-validation
    grid_search = GridSearchCV(
        classifier,
        param_grid,
        cv=2,
        dddn_jobs=-1)

    # Fit the grid search to the training data
    grid_search.fit(X_train, y_train)

    # Get the best model from the grid search
    best_model = grid_search.best_estimator_

    # Make predictions on the testing data using the best model
    y_pred = best_model.predict(X_valid)

    # Calculate accuracy score
    accuracy = accuracy_score(y_valid, y_pred)

    # Return accuracy and best hyperparameters
    return accuracy, grid_search.best_params_

"""### K fold cross validation"""

def k_fold_cross_validation(clasifier, k, X_train, y_train):

    scores = cross_val_score(clasifier, X_train, y_train, cv=k)
    mean_score = np.mean(scores)
    std_deviation = np.std(scores)
    print("Cross-Validation Scores:", scores)
    print("Mean Score:", mean_score)
    print("Standard Deviation:", std_deviation)

"""### Get prediction for classifier"""

def get_classifier_predictions(model, x_valid, y_valid):

    y_pred = model.predict(x_valid)
    accuracy = accuracy_score(y_valid, y_pred)

    return accuracy

"""# Label 1"""

train_features['label_1'].head()

plt.figure(figsize=(18, 6))
sn.countplot(data=train_label, x='label_1', color='purple')

"""## Before Feature Engineering"""

classify(models[0], 'label_1', train_features, valid_features, train_label, valid_label)

classify(models[1], 'label_1', train_features, valid_features, train_label, valid_label)

classify(models[4], 'label_1', train_features, valid_features, train_label, valid_label)

"""## Feature Engineering

### Standardization
"""

test.head()

# scaler = RobustScaler()
scaler = StandardScaler()

# Fit and transform the training data
train_standardized = scaler.fit_transform(train_features['label_1'])

# Transform the validation data using the same scaler
valid_standardized = scaler.transform(valid_features['label_1'])

# Transform the test data using the same scaler
test_standardized = scaler.transform(test_)

"""### PCA"""

threshold = 0.96

pca_train = {}
pca_valid = {}
pca_test = {}

pca = PCA(threshold, svd_solver='full')


pca_train['label_1'] = pca.fit_transform(train_standardized)
pca_valid['label_1'] = pca.transform(valid_standardized)
pca_test['label_1'] = pca.transform(test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

classify(models[0], 'label_1', pca_train, pca_valid, train_label, valid_label)

classify(models[1], 'label_1', pca_train, pca_valid, train_label, valid_label)

classify(models[4], 'label_1', pca_train, pca_valid, train_label, valid_label)

"""#### Since the higher accuracy is given by SVM we can use svm as the classifier. Then we can optimize it using hyperparameter tuning.

## Hyperparameter Tuning
"""

X_train = pca_train['label_1']
X_valid = pca_valid['label_1']
y_train = train_label['label_1']
y_valid = valid_label['label_1']

param_distributions = {
    'C': np.logspace(-3, 3, 7),
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 7)
}

# Create an instance of the classifier
classifier = SVC()

# Call the function to perform random search and get the best accuracy and hyperparameters
best_model, best_accuracy, best_hyperparameters = random_grid_search_classifier(
    classifier,
    X_train,
    y_train,
    param_distributions)

print("Best Model:", best_model)
print("Best Accuracy: %.2f%%" % (best_accuracy * 100))
print("Best Hyperparameters:", best_hyperparameters)

best_models = {}

best_models['label_1'] = best_model

accuracy = get_classifier_predictions(best_models['label_1'], X_valid, y_valid)
print("Accuracy: %.2f%%" % (accuracy * 100))

classifier = SVC(**best_hyperparameters)

k_fold_cross_validation(classifier, 5, X_train, y_train)

"""# Label 2"""

train_features['label_2'].head()

plt.figure(figsize=(18, 6))
sn.countplot(data=train_label, x='label_2', color='purple')

"""## Before Feature Engineering"""

classify(models[0], 'label_2', train_features, valid_features, train_label, valid_label)

classify(models[1], 'label_2', train_features, valid_features, train_label, valid_label)

classify(models[4], 'label_2', train_features, valid_features, train_label, valid_label)

"""## Feature Engineering

### Standardization
"""

# scaler = RobustScaler()
scaler = StandardScaler()

# Fit and transform the training data
train_standardized = scaler.fit_transform(train_features['label_2'])

# Transform the validation data using the same scaler
valid_standardized = scaler.transform(valid_features['label_2'])

# Transform the test data using the same scaler
test_standardized = scaler.transform(test_)

"""### PCA"""

threshold = 0.96
pca = PCA(threshold, svd_solver='full')


pca_train['label_2'] = pca.fit_transform(train_standardized)
pca_valid['label_2'] = pca.transform(valid_standardized)
pca_test['label_2'] = pca.transform(test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

classify(models[0], 'label_2', pca_train, pca_valid, train_label, valid_label)

classify(models[1], 'label_2', pca_train, pca_valid, train_label, valid_label)

classify(models[4], 'label_2', pca_train, pca_valid, train_label, valid_label)

"""## Hyperparameter Tuning"""

X_train = pca_train['label_2']
X_valid = pca_valid['label_2']
y_train = train_label['label_2']
y_valid = valid_label['label_2']

param_distributions = {
    'C': np.logspace(-3, 3, 7),
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 7),
    'class_weight': ['balanced']
}

# Create an instance of the classifier
classifier = SVC()

# Call the function to perform random search and get the best accuracy and hyperparameters
best_model, best_accuracy, best_hyperparameters = random_grid_search_classifier(
    classifier,
    X_train,
    y_train,
    param_distributions)

print("Best Model:", best_model)
print("Best Accuracy: %.2f%%" % (best_accuracy * 100))
print("Best Hyperparameters:", best_hyperparameters)

best_models['label_2'] = best_model

accuracy = get_classifier_predictions(best_models['label_2'], X_valid, y_valid)

print("Accuracy: %.2f%%" % (accuracy * 100.0))

classifier = SVC(**best_hyperparameters)

k_fold_cross_validation(classifier, 5, X_train, y_train)

"""# Label 3"""

train_features['label_3'].head()

x = sn.countplot(x='label_3', data=train_label)

for p in x.patches:
    x.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

sampler = RandomOverSampler(random_state=0, sampling_strategy=0.80)

ros_train_features = {}
ros_train_label = {}

ros_train_features['label_3'], ros_train_label['label_3'] = sampler.fit_resample(train_features['label_3'], train_label['label_3'])

x = sn.countplot(x='label_3', data=ros_train_label)

for p in x.patches:
    x.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

"""## Before Feature Engineering"""

classify(models[0], 'label_3', train_features, valid_features, train_label, valid_label)

classify(models[2], 'label_3', train_features, valid_features, train_label, valid_label)

classify(models[4], 'label_3', ros_train_features, valid_features, ros_train_label, valid_label)

"""## Feature Engineering

### Standardization
"""

# scaler = RobustScaler()
scaler = StandardScaler()

# Fit and transform the training data
train_standardized = scaler.fit_transform(train_features['label_3'])

# Transform the validation data using the same scaler
valid_standardized = scaler.transform(valid_features['label_3'])

# Transform the test data using the same scaler
test_standardized = scaler.transform(test_)

# standardize the random oversampled data
scaler = RobustScaler()

ros_train_standardized = scaler.fit_transform(ros_train_features['label_3'])
ros_valid_standardized = scaler.transform(valid_features['label_3'])
ros_test_standardized = scaler.transform(test_)

"""### PCA"""

threshold = 0.96
pca = PCA(threshold, svd_solver='full')


pca_train['label_3'] = pca.fit_transform(train_standardized)
pca_valid['label_3'] = pca.transform(valid_standardized)
pca_test['label_3'] = pca.transform(test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

# PCA with oversampled data
pca = PCA(threshold, svd_solver='full')

pca_ros_train = {}
pca_ros_valid = {}
pca_ros_test = {}

pca_ros_train['label_3'] = pca.fit_transform(ros_train_standardized)
pca_ros_valid['label_3'] = pca.transform(ros_valid_standardized)
pca_ros_test['label_3'] = pca.transform(ros_test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for oversampled train data:", retained_features_len)

classify(models[0], 'label_3', pca_train, pca_valid, train_label, valid_label)

classify(models[2], 'label_3', pca_train, pca_valid, train_label, valid_label)

classify(models[4], 'label_3', pca_ros_train, pca_ros_valid, ros_train_label, valid_label)

"""## Hyperparameter Tuning"""

X_train = pca_train['label_3']
X_valid = pca_valid['label_3']
y_train = train_label['label_3']
y_valid = valid_label['label_3']

param_distributions = {
    'C': np.logspace(-3, 3, 7),
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 7),
    'class_weight': ['balanced']
}

# Create an instance of the classifier
classifier = SVC()

# Call the function to perform random search and get the best accuracy and hyperparameters
best_model, best_accuracy, best_hyperparameters = random_grid_search_classifier(
    classifier,
    X_train,
    y_train,
    param_distributions)

print("Best Model:", best_model)
print("Best Accuracy: %.2f%%" % (best_accuracy * 100))
print("Best Hyperparameters:", best_hyperparameters)

best_models['label_3'] = best_model

accuracy = get_classifier_predictions(best_models['label_3'], X_valid, y_valid)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

classifier = SVC(**best_hyperparameters)

k_fold_cross_validation(classifier, 5, X_train, y_train)

"""# Label 4"""

train_features['label_4'].head()

plt.figure(figsize=(18, 6))
sn.countplot(data=train_label, x='label_4', color='purple')

sampler = RandomOverSampler(random_state=0)

ros_train_features['label_4'], ros_train_label['label_4'] = sampler.fit_resample(train_features['label_4'], train_label['label_4'])

plt.figure(figsize=(18, 6))
sn.countplot(data=ros_train_label, x='label_4', color='purple')

"""## Before Feature Engineering"""

classify(models[0], 'label_4', train_features, valid_features, train_label, valid_label)

classify(models[1], 'label_4', train_features, valid_features, train_label, valid_label)

classify(models[4], 'label_4', ros_train_features, valid_features, ros_train_label, valid_label)

"""## Feature Engineering

### Standardization
"""

scaler = StandardScaler()

# Fit and transform the training data
train_standardized = scaler.fit_transform(train_features['label_4'])

# Transform the validation data using the same scaler
valid_standardized = scaler.transform(valid_features['label_4'])

# Transform the test data using the same scaler
test_standardized = scaler.transform(test_)

# standardize the random oversampled data
scaler = RobustScaler()

ros_train_standardized = scaler.fit_transform(ros_train_features['label_4'])
ros_valid_standardized = scaler.transform(valid_features['label_4'])
ros_test_standardized = scaler.transform(test_)

"""### PCA"""

threshold = 0.96
pca = PCA(threshold, svd_solver='full')


pca_train['label_4'] = pca.fit_transform(train_standardized)
pca_valid['label_4'] = pca.transform(valid_standardized)
pca_test['label_4'] = pca.transform(test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for train data:", retained_features_len)

# PCA with oversampled data
pca = PCA(threshold, svd_solver='full')

pca_ros_train = {}
pca_ros_valid = {}
pca_ros_test = {}

pca_ros_train['label_4'] = pca.fit_transform(ros_train_standardized)
pca_ros_valid['label_4'] = pca.transform(ros_valid_standardized)
pca_ros_test['label_4'] = pca.transform(ros_test_standardized)

explained_variance = pca.explained_variance_ratio_

retained_features_len = len(explained_variance)
print("Number of componenets for oversampled train data:", retained_features_len)

classify(models[0], 'label_4', pca_train, pca_valid, train_label, valid_label)

classify(models[1], 'label_2', train_features, valid_features, train_label, valid_label)

classify(models[4], 'label_4', pca_ros_train, pca_ros_valid, ros_train_label, valid_label)

"""## Hyper Parameter Tuning"""

X_train = pca_train['label_4']
X_valid = pca_valid['label_4']
y_train = train_label['label_4']
y_valid = valid_label['label_4']

param_distributions = {
    'C': np.logspace(-3, 3, 7),
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': np.logspace(-3, 3, 7),
    'class_weight': ['balanced']
}

# Create an instance of the classifier
classifier = SVC()

# Call the function to perform random search and get the best accuracy and hyperparameters
best_model, best_accuracy, best_hyperparameters = random_grid_search_classifier(
    classifier,
    X_train,
    y_train,
    param_distributions)

print("Best Model:", best_model)
print("Best Accuracy: %.2f%%" % (best_accuracy * 100))
print("Best Hyperparameters:", best_hyperparameters)

best_models['label_4'] = best_model

accuracy = get_classifier_predictions(best_models['label_4'], X_valid, y_valid)

print("Accuracy: %.2f%%" % (accuracy * 100.0))

# cross validation
classifier = SVC(**best_hyperparameters)

k_fold_cross_validation(classifier, 5, X_train, y_train)

"""## Prediction"""

y_preds = {}

for label in labels:
    y_preds[label] = best_models[label].predict(pca_test[label])

id_list = test['ID']

result = {
    'ID': id_list,
    'label_1': y_preds['label_1'],
    'label_2': y_preds['label_2'],
    'label_3': y_preds['label_3'],
    'label_4': y_preds['label_4']
}

result_ = pd.DataFrame(result)

result_.to_csv('layer_9.csv', index=False)